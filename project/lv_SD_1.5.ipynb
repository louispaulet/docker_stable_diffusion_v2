{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKVPm8Fo9Zh7"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functorch import vmap\n",
    "x = torch.randn(3)\n",
    "y = vmap(torch.sin)(x)\n",
    "assert torch.allclose(y, x.sin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "!python -m xformers.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAN5CI5-BlXp"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler.save_pretrained('scheduler/EulerDiscreteScheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = EulerDiscreteScheduler.from_pretrained(\"scheduler/EulerDiscreteScheduler\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote model path\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# local model path\n",
    "model_id = \"models/stable-diffusion-v1.5\"\n",
    "\n",
    "\n",
    "\n",
    "# Use the Euler scheduler here instead\n",
    "# scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id,scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16)\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(0)\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.safety_checker = (lambda images, clip_input: (images, False))\n",
    "pipe.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model locally after initial download\n",
    "# pipe.save_pretrained('models/stable-diffusion-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nfxOjVrFq-9"
   },
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "from time import time\n",
    "\n",
    "# prompt = \"low angle 50mm photograph of a majestic bacon-cheesehamburger composed of grilled beef patties and fresh salad and tomato and secret sauce and melted american cheese and sesame seed buns, 4k award winning food photography, trending on artstation\"\n",
    "# prompt = \"plateau de fromages frais et affinés 999€\"\n",
    "\n",
    "# prompt = \"La religieuse est composée de deux choux posés l'un sur l'autre, dont le chou supérieur, qui est censé représenter la tête, est deux fois plus petit.Les choux sont recouverts de fondant du même parfum que la crème pâtissière, et une crème au beurre généralement parfumée au café ou à la vanille permet de tenir la tête.\"\n",
    "\n",
    "# prompt = \"Religieuse is a French pastry, 999€\"\n",
    "\n",
    "# prompt = \"top view picture, of a michelin star restaurant dish, award winning food photography, 4k food picture, french signature dish, silver cutlery on each side of the plate, ornated porcelain plate with floral patterns and gilded meandros, crystal drinking glass with red wine, everything is resting on a marble table, fresh flower arrangement in an ornate vase, ming vase holding flowers\"\n",
    "\n",
    "\n",
    "# prompt = \"masterwork oil on canvas still life painting of a cheeseburger banquet by Caravaggio,masterwork oil on canvas still life painting of a cheeseburger feast by Caravaggio, juicy ground beef patty, sesame seed bun, melted american cheese, tomato and pickle slices, trending on artstation, red drape in the background, flower bouquet in ornate flower pot, silver cutlery, ornate vase\"\n",
    "\n",
    "\n",
    "# prompt = \"masterwork oil on canvas still life painting of a cheeseburger feast by Caravaggio, the cheeseburger is composed of juicy ground beef patty sesame seed bun melted american cheese tomato pickle slices by Caravaggio, trending on artstation, red drape in the background and flower bouquet in ornate flower pot and silver cutlery and ornate vase by Caravaggio\"\n",
    "\n",
    "prompt = \"masterwork oil on canvas still life painting of a smartphone laying on a table by Caravaggio, smartphone displaying lockscreen by Caravaggio, a amrtphone still life painting by Caravaggio\"\n",
    "# neg_prompt = \"blurry, out of focus, frame, framed, new artist, glossy, recent, unknown\"\n",
    "neg_prompt = \"frame, blurry\"\n",
    "\n",
    "set_guidance_scale = 7.5\n",
    "\n",
    "def txt2img(prompt, neg_prompt, n_iterations, display_image, in_num_inference_steps, input_seed):\n",
    "    \n",
    "    seed_generator = torch.Generator('cuda').manual_seed(input_seed)\n",
    "    image = None\n",
    "    for i in range(n_iterations):\n",
    "        with autocast('cuda'):\n",
    "          image = pipe(prompt, negative_prompt=neg_prompt, height=512, width=512,generator=seed_generator, guidance_scale = set_guidance_scale, num_inference_steps = in_num_inference_steps).images[0]\n",
    "          image.save(f\"images/{slugify(prompt)[:100]}_{i}_{time()}.png\")\n",
    "        if display_image:\n",
    "            display(image)\n",
    "\n",
    "    # return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "prime_list = []\n",
    "\n",
    "for num in range(400, 500):\n",
    "    if all(num%i!=0 for i in range(2,int(math.sqrt(num))+1)):\n",
    "        prime_list.append(num)\n",
    "        \n",
    "print(len(prime_list))\n",
    "prime_list = prime_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = prime_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG_aywiYDoRv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for curr_seed in seed_list:\n",
    "    image = txt2img(prompt,neg_prompt, 1, True, 25, curr_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
